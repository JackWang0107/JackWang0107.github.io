---
title: 炼丹手记
date: 2022-02-02 00:09:08
img: https://jack-1307599355.cos.ap-shanghai.myqcloud.com/img/image-20220202001104734.png
summary: '本文记录了我炼丹的心得'
categories:
  - Deep Learning Blogs
tags:
  - Deep Learning
  - Pytorch
  - AI
  - Python
---

> 本文是我在炼丹的过程中的一些心得

![Pytorch官网](https://jack-1307599355.cos.ap-shanghai.myqcloud.com/img/image-20220202001104734.png)





# 炼丹手记

因为目前是一个本科生，所以经常需要balance课程、科研、竞赛、写代码……所以一个问题就是我经常一段时间忙于做事情A，一段时间忙于做事情B。

这个对于炼丹其实是不太好的，因为炼丹的经验会随着时间的流逝而遗忘，因此决定还是把自己的炼丹时候的心得都记录下来，这样的话方便以后回顾、总结、提高。

此外，因为我用的是Pytorch，所以有一些关于Pytorch的心得，这部分工具的心得可能不是很适合TensorFlow用户，因此略掉就好。









## 1. Pytorch读取Numpy

有的时候数据是以numpy的npy或者npz形式保存的，这个时候直接使用`np.load`就可以读取了。可是读取完了之后从numpy的ndarray转成torch的tensor的时候会有问题。

具体来说就是首先是数据类型的问题：

- numpy里转数据类型不如pytorch方便
- pytorch有的时候在计算的时候是需要统一数据类型为float的，因此用numpy转换数据类型和用torch转换数据类型混着来容易分不清楚
- 有的时候pytorch计算有需要LongTensor，例如用交叉熵/负对数似然损失函数，因为去nature log的时候数值的范围比较大

**因此，如果数据是numpy的文件的话，读完了之后直接转torch的tensor再进行后续操作**

读的时候`np.load`，而`torch.from_numpy`会直接帮我们完成从`np.ndarrary`到`torch.Tensor`的转换，转换过程中保持数据格式、内存对齐等属性不变，然后我们再用`torch.Tensor.long()`之类的方法转换

例如：

```python
class ExampleDataset(Dataset):
	def __init__(self):
        data = torch.from_numpy(np.load(file_path)).float()
        # 类似的，还可以是
        long = torch.from_numpy(np.load(file_path)).long()
```





## 2. Pytorch使用CrossEntropy

CrossEntropy损失函数一般用于分类任务。具体的原理就是交叉熵（Cross-Entropy）其实等价于最大化似然（Maximize Likelihood），或者说最小化对数负似然（Negative Log Likelihood），即让网络输出的分布和真实的数据的分布越相似越好。因此，交叉熵损失（Cross-Entropy Loss）其实等于负对数似然（NLL Loss）

在Pytorch中，提供了负对数似然损失函数的API，然后在其基础上又集成了softmax，就成了Pytorch中的Cross-Entropy损失函数。因此一般在分类的时候都是直接使用Cross-Entropy作为损失函数的，就避免了我们自己写softmax。

但是使用Cross-Entropy的时候因为要计算SoftMax，所以Pytorch要求输入的`target`都是long类型的

所以最好在读取完数据之后把`target`，也就是y转为long，即

```python
# train
self.network.train()
for step, (x, y) in enumerate(self.train_loader):
    # 注意在这里转换，如果Dataset里面以及转换了这里就不要转换了
    x, y = x.to(dtype=self.network.dtype, device=self.available_device), y.to(
        device=self.available_device)
    
    # zero grad
    self.network.zero_grad()
    
    # calculate loss
    output = self.network(x)
    
    # 我这里是在Dataset中就已经转为long了，所以这里保持不变即可
    loss: torch.Tensor = self.loss_function(output, y)
        
    # Gradient Descent
    loss.backward()
    
    # Step
    self.optimizer.step()
```







## 3. 训练代码调bug

把所有的测试代码写完了之后一般来说是没法直接开始训练的，需要调一下bug，但是这个时候因为`epoch`、`dataloader`的循环都写好了，因此如果直接调试的话可能会卡到训练部分。

这个时候可以给训练循环的这类步骤直接break掉就行了，因为的目的在于验证流程，即验证是否可以跑完流程而非开始训练，因此这个时候break掉即可。即

```python
for epoch in range(n_epoch):
    x: torch.Tensor
    y: torch.Tensor

    # train
    self.network.train()
    for step, (x, y) in tqdm(enumerate(self.train_loader)):
        x, y = x.to(dtype=self.network.dtype, device=self.available_device), y.to(
            device=self.available_device)
        # zero grad
        self.network.zero_grad()
        # calculate loss
        output = self.network(x)
        loss: torch.Tensor = self.loss_function(output, y)
        # Gradient Descent
        loss.backward()
        # Step
        self.optimizer.step()
        # summary in train
        self.writer.add_scalar(tag="loss/train", scalar_value=loss.item(),
                               global_step=epoch * len(self.train_loader) + step)
        # 注意，调试的时候break即可
        break

    # validation
    self.network.eval()
    with torch.no_grad():
        for step, (x, y) in tqdm(enumerate(self.val_loader)):
            x, y = x.to(dtype=self.network.dtype, device=self.available_device), y.to(
                device=self.available_device)
            # calculate loss
            output = self.network(x)
            loss = self.loss_function(output, y)
            # summary in validation
            self.writer.add_scalar(tag="loss/test", scalar_value=loss.item(),
                                           global_step=epoch * len(self.val_loader) + step)
                    

            #同上，调试的时候break掉
            break
            
    # early stop
    if loss < min_val_loss:
        min_val_loss = loss
        earl_stop = 0
        self.save_check_point()
    else:
        earl_stop += 1

    if earl_stop > self.early_stop_cnt:
        print(f"{Fore.YELLOW}Early Stoped at epoch: {epoch}")
        break

    # print logs
    print(f"Epoch: {Fore.GREEN + Style.BRIGHT}{epoch}/{n_epoch}{Style.RESET_ALL}, "
        "val_loss: {Fore.GREEN + Style.BRIGHT}{loss:>.5f}{Style.RESET_ALL}, "
        "min_val_loss: {Fore.GREEN + Style.BRIGHT}{min_val_loss:>.5f}{Style.RESET_ALL}")
```

