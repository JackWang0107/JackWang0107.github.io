---
title: 炼丹手记
date: 2022-02-02 00:09:08
img: https://jack-1307599355.cos.ap-shanghai.myqcloud.com/img/image-20220202001104734.png
summary: '本文记录了我炼丹的心得'
categories:
  - Deep Learning Blogs
tags:
  - Deep Learning
  - Pytorch
  - AI
  - Python
---

> 本文是我在炼丹的过程中的一些心得

![Pytorch官网](https://jack-1307599355.cos.ap-shanghai.myqcloud.com/img/image-20220202001104734.png)





# 炼丹手记

因为目前是一个本科生，所以经常需要balance课程、科研、竞赛、写代码……所以一个问题就是我经常一段时间忙于做事情A，一段时间忙于做事情B。

这个对于炼丹其实是不太好的，因为炼丹的经验会随着时间的流逝而遗忘，因此决定还是把自己的炼丹时候的心得都记录下来，这样的话方便以后回顾、总结、提高。

此外，因为我用的是Pytorch，所以有一些关于Pytorch的心得，这部分工具的心得可能不是很适合TensorFlow用户，因此略掉就好。









## 1. Pytorch读取Numpy

有的时候数据是以numpy的npy或者npz形式保存的，这个时候直接使用`np.load`就可以读取了。可是读取完了之后从numpy的ndarray转成torch的tensor的时候会有问题。

具体来说就是首先是数据类型的问题：

- numpy里转数据类型不如pytorch方便
- pytorch有的时候在计算的时候是需要统一数据类型为float的，因此用numpy转换数据类型和用torch转换数据类型混着来容易分不清楚
- 有的时候pytorch计算有需要LongTensor，例如用交叉熵/负对数似然损失函数，因为去nature log的时候数值的范围比较大

**因此，如果数据是numpy的文件的话，读完了之后直接转torch的tensor再进行后续操作**

读的时候`np.load`，而`torch.from_numpy`会直接帮我们完成从`np.ndarrary`到`torch.Tensor`的转换，转换过程中保持数据格式、内存对齐等属性不变，然后我们再用`torch.Tensor.long()`之类的方法转换

例如：

```python
class ExampleDataset(Dataset):
	def __init__(self):
        data = torch.from_numpy(np.load(file_path)).float()
        # 类似的，还可以是
        long = torch.from_numpy(np.load(file_path)).long()
```





## 2. Pytorch使用CrossEntropy

CrossEntropy损失函数一般用于分类任务。具体的原理就是交叉熵（Cross-Entropy）其实等价于最大化似然（Maximize Likelihood），或者说最小化对数负似然（Negative Log Likelihood），即让网络输出的分布和真实的数据的分布越相似越好。因此，交叉熵损失（Cross-Entropy Loss）其实等于负对数似然（NLL Loss）

在Pytorch中，提供了负对数似然损失函数的API，然后在其基础上又集成了softmax，就成了Pytorch中的Cross-Entropy损失函数。因此一般在分类的时候都是直接使用Cross-Entropy作为损失函数的，就避免了我们自己写softmax。



### 1. 数据类型的问题

但是使用Cross-Entropy的时候因为要计算Softmax，所以Pytorch要求输入的`target`是long类型的，即输入的label要求是long，输入的`input`也就是预测值是float

所以最好在读取完数据之后把`target`，也就是y转为long，y_pred转为float，即

```python
# train
self.network.train()
for step, (x, y) in enumerate(self.train_loader):
    # 注意在这里转换，如果Dataset里面以及转换了这里就不要转换了
    x, y = x.to(dtype=self.network.dtype, device=self.available_device), y.to(
        device=self.available_device)
    
    # zero grad
    self.network.zero_grad()
    
    # calculate loss
    output = self.network(x)
    
    # 注意这里要转成float和long
    loss: torch.Tensor = self.loss_function(output.float(), y.long())
        
    # Gradient Descent
    loss.backward()
    
    # Step
    self.optimizer.step()
```

否则会出现如下的报错

![没有转成long的错误](https://jack-1307599355.cos.ap-shanghai.myqcloud.com/image-20220317100142950.png)



### 2. 输入顺序问题

此外，除了要求转成long，计算CrossEntropyLoss的时候是要求预测值作为第一个实参，真实的label作为第二个实参，即

```python
# train
self.network.train()
for step, (x, y) in enumerate(self.train_loader):
    # 注意在这里转换，如果Dataset里面以及转换了这里就不要转换了
    x, y = x.to(dtype=self.network.dtype, device=self.available_device), y.to(
        device=self.available_device)
    
    # zero grad
    self.network.zero_grad()
    
    # calculate loss
    output = self.network(x)
    
    # 注意第一位是预测值，第二位是long的label
    loss: torch.Tensor = self.loss_function(output, y.long())
        
    # Gradient Descent
    loss.backward()
    
    # Step
    self.optimizer.step()
```

否则就会出现下面的错误

![顺序出错的报错](https://jack-1307599355.cos.ap-shanghai.myqcloud.com/image-20220317100515724.png)



### 3. 输入维度的问题

对于分类问题来说，我们输入的$x$最后得到的$y_{pred}$是$[Batch, Class]$类型的数据，因此Pytoch的CrossEntropyLoss要求输入的预测值的是二维的数据。但是Pytorch却要求GroundTruth的$y$的形状是一维的，长度和$Batch$数相等，每一位表示对应的类别，即

```python
# train
self.network.train()
for step, (x, y) in enumerate(self.train_loader):
    # 注意在这里转换，如果Dataset里面以及转换了这里就不要转换了
    x, y = x.to(dtype=self.network.dtype, device=self.available_device), y.to(
        device=self.available_device)
    
    # zero grad
    self.network.zero_grad()
    
    # calculate loss
    output = self.network(x)
    
    # 注意对ground truth进行了squeeze来保证是一维的，因为dataloader里面进行了stack
    loss: torch.Tensor = self.loss_function(output, y.squeeze().long())
        
    # Gradient Descent
    loss.backward()
    
    # Step
    self.optimizer.step()
```

否则会报错	

![形状出错的报错](https://jack-1307599355.cos.ap-shanghai.myqcloud.com/image-20220317101634866.png)



## 3. 训练代码调bug

把所有的测试代码写完了之后一般来说是没法直接开始训练的，需要调一下bug，但是这个时候因为`epoch`、`dataloader`的循环都写好了，因此如果直接调试的话可能会卡到训练部分。

这个时候可以给训练循环的这类步骤直接break掉就行了，因为的目的在于验证流程，即验证是否可以跑完流程而非开始训练，因此这个时候break掉即可。即

```python
for epoch in range(n_epoch):
    x: torch.Tensor
    y: torch.Tensor

    # train
    self.network.train()
    for step, (x, y) in tqdm(enumerate(self.train_loader)):
        x, y = x.to(dtype=self.network.dtype, device=self.available_device), y.to(
            device=self.available_device)
        # zero grad
        self.network.zero_grad()
        # calculate loss
        output = self.network(x)
        loss: torch.Tensor = self.loss_function(output, y)
        # Gradient Descent
        loss.backward()
        # Step
        self.optimizer.step()
        # summary in train
        self.writer.add_scalar(tag="loss/train", scalar_value=loss.item(),
                               global_step=epoch * len(self.train_loader) + step)
        # 注意，调试的时候break即可
        break

    # validation
    self.network.eval()
    with torch.no_grad():
        for step, (x, y) in tqdm(enumerate(self.val_loader)):
            x, y = x.to(dtype=self.network.dtype, device=self.available_device), y.to(
                device=self.available_device)
            # calculate loss
            output = self.network(x)
            loss = self.loss_function(output, y)
            # summary in validation
            self.writer.add_scalar(tag="loss/test", scalar_value=loss.item(),
                                           global_step=epoch * len(self.val_loader) + step)
                    

            #同上，调试的时候break掉
            break
            
    # early stop
    if loss < min_val_loss:
        min_val_loss = loss
        earl_stop = 0
        self.save_check_point()
    else:
        earl_stop += 1

    if earl_stop > self.early_stop_cnt:
        print(f"{Fore.YELLOW}Early Stoped at epoch: {epoch}")
        break

    # print logs
    print(f"Epoch: {Fore.GREEN + Style.BRIGHT}{epoch}/{n_epoch}{Style.RESET_ALL}, "
        "val_loss: {Fore.GREEN + Style.BRIGHT}{loss:>.5f}{Style.RESET_ALL}, "
        "min_val_loss: {Fore.GREEN + Style.BRIGHT}{min_val_loss:>.5f}{Style.RESET_ALL}")
```









## 4. 训练阶段Early Stop

在训练网络的过程中，可能会出现过拟合，因此就需要用Early Stop技术来防止过拟合。判断是否过拟合的依据是看训练和验证阶段的loss曲线，但在真实训练的时候是没办法人工一直查看损失曲线的，因此我们更希望在代码里面能够进行判断。

相比于记录下来每一个epoch的性能，Early Stop其实更加简单粗暴，**Early Stop指的其实就是如果网络在N个epoch中性能都没有下降，那么就停止训练**。

在代码里的实现类似这样

```python
for epoch in range(n_epoch):
    x: torch.Tensor
    y: torch.Tensor

    # train
    self.network.train()
    for step, (x, y) in tqdm(enumerate(self.train_loader)):
        pass

    # validation
    self.network.eval()
    with torch.no_grad():
        for step, (x, y) in tqdm(enumerate(self.val_loader)):
            pass
            
    # early stop
    if loss < min_val_loss:
        min_val_loss = loss
        earl_stop = 0
        self.save_check_point()
    else:
        earl_stop += 1

    if earl_stop > self.early_stop_cnt:
        print(f"{Fore.YELLOW}Early Stoped at epoch: {epoch}")
        break
```







## 5. 代码里使用SummaryWriter

如果代码里使用了SummaryWriter，那么调试的时候有一个问题，就是Python是遇到报错之后就直接退出，不会运行完所有的代码。

而SummaryWriter是一个异步的服务程序，因此在使用之后是需要关闭的。如果这次调试没有关闭SummaryWriter，那么下一次调试产生的数据就会写入到这次调试的数据里去。

所以在调试阶段为了防止这个问题，可以吧SummaryWriter的关闭方法写入到类的`__del__`方法里去。`__del__`是在对象被销毁的时候运行，一般在程序运行结束（不管是正常结束还是因为异常报错）的时候，Python会自动销毁所有的对象。

因此如果我们把训练的代码写成了一个类的话，重写`__del__`方法就可以帮助我们关闭SummaryWriter

具体在代码中是这样的

```python
class NetworkTrainer:
    available_device = "cuda:0" if torch.cuda.is_available() else "cpu"
    def __init__(self):
        self.writer = SummaryWriter(log_dir=path2log)
        pass
    
    def __del__(self):
        self.writer.close()
```







## 6. 高效使用tqdm

在训练代码的时候，往往使用tqdm来给出来一个训练的进度条，例如下面

![tqdm可视化得到的训练进度条](https://jack-1307599355.cos.ap-shanghai.myqcloud.com/697264-20200723170905473-1915244785.png)

然而很多时候我们都需要再输出一些内容，比如训练和验证的loss，分类的准确率这类信息。

如果我们直接print的话，就会导致tqdm的进度条断开，非常的不美观，因此我们可以使用tqdm的`set_description`或者`set_postfix`方法来设置一些训练的信息，可是这样做并不优雅，因为能够显示的信息有限。一个优雅的解决方案如下面的gif所示

![优雅的解决方案](https://jack-1307599355.cos.ap-shanghai.myqcloud.com/深度录屏_x-terminal-emulator_20220317111046.gif)

即每次输出都会保证tqdm的进度条在最下面。要实现这样的效果，其实通过tqdm的另外一个方法就行了。

在输出的时候，缓冲区是被tqdm占用的，因此直接输出肯定会有问题，我们使用tqdm的write方法来让tqdm自动处理即可。例如实现上面的效果的代码

```python
def train(self, n_epoch: int, early_stop: int):
        x: torch.Tensor
        y: torch.Tensor
        max_acc = 0
        early_stop_cnt = 0
        for epoch in (tt := tqdm.trange(n_epoch)):
            # train
            for step, (x, y) in enumerate(self.train_loader):
                x, y = x.to(device=self.available_device, dtype=self.network.dtype), y.to(device=self.available_device,
                                                                                          dtype=self.network.dtype)
                pass

            # val
            acc = 0
            all = 0
            with torch.no_grad():
                for step, (x, y) in enumerate(self.val_loader):
                    x, y = x.to(device=self.available_device, dtype=self.network.dtype), y.to(
                        device=self.available_device,
                        dtype=self.network.dtype)
                    pass
                
                    # get acc
                    acc += (y_pred.argmax(dim=1) == y.squeeze()).sum()
                    all += x.shape[0]
            self.writer.add_scalar(tag="acc", scalar_value=(cur_acc := acc / all), global_step=epoch)

            # early stop
            if cur_acc > max_acc:
                max_acc = cur_acc
                early_stop_cnt = 0
                if not self.checkpoint_path.parent.exists():
                    self.checkpoint_path.parent.mkdir(parents=True)
                torch.save(self.network.state_dict(), self.checkpoint_path)
            else:
                early_stop_cnt += 1
            
            # 注意，这里调用了tqdm的trange对象的write方法，其实别的对象也是可以的
            tt.write(
                f"Epoch [{epoch:>5d}|{n_epoch:>5d}], train_loss {train_loss:>7.4f}, val_loss {val_loss:>7.4f}, "
                f"early_stop_cnt: [{early_stop_cnt:>5d}|{early_stop:>5d}]")
            if early_stop_cnt >= early_stop:
                break
```







## 7. 网络中含有线性层的问题

很多时候网络中都需要写线性层，尤其是对于分类网络或者Self-Attention的结构来说。在网络中经常出现的一个情况就是前面的层不是线性层，而后面接的是线性层。这个时候就需要注意一下特征数量的问题。

以图像分类的卷积网络为例，输入要求的形状是：$[Batch, Channel, Width, Height]$，经过前面几个用于特征提取的层之后，feature map的形状是：$[Batch, Channel_{new}, Width_{new}, Height_{new}]$。但是对于全连接层来说，他其实干的事情就是矩阵相乘，因此要求输入的形状是$[Batch,feature\_num]$。所以**直接把前面层的输出丢给线性层往往是会有问题的，这个时候就需要对前面层的输出进行变换**。

对于上面的分类的卷积网络来说，就需要把$Batch$维度之后的$Channel_{new}$、$Width_{new}$和$Height_{new}$三个维度拉平成一个向量丢给分类网络。

```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.conv1(x)
        x = self.pool1(x)
        x = self.activation(x)
        x = self.conv2(x)
        x = self.pool2(x)
        x = self.activation(x)
        # 注意这里把后面的特征维度拉平为一个向量
        x = x.flatten(start_dim=1)
        x = self.linear1(x)
        x = self.activation(x)
        x = self.linear2(x)
        x = self.activation(x)
        return self.output(x)
```

因此，对于不同的网络模型，在进行操作的时候一定要注意，在经过线性层之前一定要把上一层的输入处理好。





## 8. TensorBoard查看多次实验

TensorBoard实际上是支持一次性查看多组实验的训练过程的。多组实验的结果会显示在左下角，我们点击就可以切换

![TensorBoard支持查看多组实验](https://jack-1307599355.cos.ap-shanghai.myqcloud.com/20180726171253594)

但是我们自己在查看的时候，很多情况下都是左下角只有一个`.`，并且只会显示一组实验的过程。

<img src="https://jack-1307599355.cos.ap-shanghai.myqcloud.com/image-20220318160601709.png" alt="TensorBoard支持查看多组实验" style="zoom:67%;" />

其实这是由于我们的路径选择的问题，TensorBoard会递归的扫描我们指定的目录下所有的文件，找到其中所有的event文件，然后把这个event文件所在的文件夹视为一个训练，因此我们其实只需要在启动TensorBoard的时候制定所有训练日志所在的父级目录即可，例如下面

![制定打开父级文件即可](https://jack-1307599355.cos.ap-shanghai.myqcloud.com/image-20220318161426776.png)







## 9. 从Tensor列表中创建Tensor

在Dataset读取图像的时候，进场需要进行的一个从操作就是读取图像，这个时候往往读出来的是单张图像的Tensor。我们接下来会使用一个list，来保存所有的单张图像的Tensor。在读取完接下来我们又会需要把所有的图像转换成一个大的TensorBoard，即`[Batch, Channel, Width, Height]`形状的TensorBoard。

这个时候我们就需要把`List[Tensor[Channel, Width, Height]]`的列表转换为`Tensor[Batch, Channel, Width, Height]`。如果直接使用`torch.Tensor(List[Tensor])`的话会报错

![会抛出来ValueError](https://jack-1307599355.cos.ap-shanghai.myqcloud.com/image-20220321230351403.png)

这个时候我们需要用的其实是`torch.stack`，他会自动帮我们在前面构建一个维度，即

```python
a: List[torch.Tensor]
b = torch.stack(a)
```

其实，`a`只需要是一个以`torch.Tensor`为原子的可迭代对象即可







## 10. 线性层flatten问题

前面说到，对于分类任务来说，在输入到网络进行计算的时候在线性层之前需要把输入flatten。这个时候存在一个问题，就是flatten的时候得到的feature的维度数量是和输入的时候的图像的大小是相关的。

因此在网络初始化的时候就需要有一个参数指定这个feature的数量。具体feature的数量完全可以单步调试的时候来确定，即

```python
class Net(nn):
    def __init__(self, in_size: int, predict_class: int):
        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=(11, 11), stride=4),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(3, 3), stride=2)
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=(5, 5), stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(3, 3), stride=2)
        )
        # 注意这里留下来了in_size
        self.linear1 = nn.Sequential(
            nn.Linear(in_features=in_size, out_features=4096),
            nn.ReLU(),
            nn.Dropout(p=0.5)
        )
        self.linear2 = nn.Sequential(
            nn.Linear(in_features=4096, out_features=4096),
            nn.ReLU(),
            nn.Dropout(p=0.5)
        )
        self.output = nn.Linear(in_features=4096, out_features=predict_class)
```







## 11. 持续监控GPU缓存

在训练模型的时候我们需要监控内存和GPU缓存的用量。内存的用量使用htop就可以很好的来监控，但是GPU缓存的话使用`nvidia-smi`只能看到当前时刻的用量。

![htop可以很好的监控系统状态，包括内存](https://jack-1307599355.cos.ap-shanghai.myqcloud.com/image-20220322105449663.png)

那么我们就会想问有没有类似htop、top一类可以实时监控GPU缓存用量的命令行工具呢？

答案其实是有的，下面几种方法都可以实现持续不断地监控GPU缓存的用量





### 1. watch nvidia-smi

第一种方法就是最原始`watch`+`nvidia-smi`的方式，使用下面的命令，指定0.1秒查询一次

```shell
watch -n0.1 nvidia-smi
```

使用效果如下

![watch+nvidia-smi的使用效果](https://jack-1307599355.cos.ap-shanghai.myqcloud.com/深度录屏_选择区域_20220322105816.gif)





### 2. gpustat

使用`nvidia-smi`得到的输出是没有颜色的，我们就希望能不能像htop一样有彩色的输出。这个时候我们就可以结束`gpustat`这个工具实现。

首先安装该工具

```shell
pip install gpustat
```

然后在安装的python环境下，使用如下命令

```shell
gpustat -cp --watch
```

就可以看到如下的效果

![gpustat的效果](https://jack-1307599355.cos.ap-shanghai.myqcloud.com/深度录屏_选择区域_20220322110301.gif)
